{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df = pl.read_parquet(\"dataframes/preprocessed.parquet\")\n",
    "\n",
    "df = df.with_columns(\n",
    "\tpl.col(\"clean_content\")\n",
    "\t.str.to_lowercase()\n",
    "\t.str.replace_all(r'[^\\w\\s]', ' ')\n",
    "\t.str.replace_all(r'\\s+', ' ')\n",
    "\t.alias(\"clean_content\")\n",
    ")\n",
    "\n",
    "train_df, test_df = train_test_split(df[\"clean_content\", \"target\"], test_size=0.3, random_state=42)\n",
    "\n",
    "train_data = list(zip(train_df[\"clean_content\"], train_df[\"target\"]))\n",
    "test_data = list(zip(test_df[\"clean_content\"], test_df[\"target\"]))\n",
    "\n",
    "data = [row[0].split() for row in train_data]\n",
    "data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "EMBEDDING_DIM = 300\n",
    "\n",
    "w2v_model = Word2Vec(\n",
    "\tsentences=data,\n",
    "\tvector_size=EMBEDDING_DIM,\n",
    "\twindow=5,\n",
    "\tmin_count=5,\n",
    "\tworkers=-1,\n",
    "\tsg=1\n",
    ")\n",
    "\n",
    "W2V_VOCAB_DICT: dict[str, int] = {str(word): i + 2 for i, word in enumerate(w2v_model.wv.index_to_key)}\n",
    "\n",
    "W2V_VOCAB_DICT['<PAD>'] = 0\n",
    "W2V_VOCAB_DICT['<UNK>'] = 1\n",
    "\n",
    "W2V_VOCAB_SIZE = len(W2V_VOCAB_DICT)\n",
    "\n",
    "print(f\"Taille finale du vocabulaire (incluant <PAD>/<UNK>): {W2V_VOCAB_SIZE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import FastText\n",
    "\n",
    "ft_model = FastText(\n",
    "    sentences=data,\n",
    "    vector_size=EMBEDDING_DIM,\n",
    "\twindow=5,\n",
    "\tmin_count=5,\n",
    "\tworkers=-1,\n",
    "\tsg=1\n",
    ")\n",
    "\n",
    "FT_VOCAB_DICT: dict[str, int] = {str(word): i + 2 for i, word in enumerate(ft_model.wv.index_to_key)}\n",
    "\n",
    "FT_VOCAB_DICT['<PAD>'] = 0\n",
    "FT_VOCAB_DICT['<UNK>'] = 1\n",
    "\n",
    "FT_VOCAB_SIZE = len(FT_VOCAB_DICT)\n",
    "\n",
    "print(f\"Taille finale du vocabulaire (incluant <PAD>/<UNK>): {FT_VOCAB_SIZE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQ_LEN = 15\n",
    "\n",
    "def text_to_sequence(text, vocab_dict):\n",
    "\ttext = text.split(\" \")\n",
    "\n",
    "\tsequence = [vocab_dict.get(token, vocab_dict['<UNK>']) for token in text]\n",
    "\n",
    "\tif len(sequence) < MAX_SEQ_LEN:\n",
    "\t\tsequence += [vocab_dict['<PAD>']] * (MAX_SEQ_LEN - len(sequence))\n",
    "\telse:\n",
    "\t\tsequence = sequence[:MAX_SEQ_LEN]\n",
    "\treturn sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "X_train_w2v = np.array([text_to_sequence(tweet, W2V_VOCAB_DICT) for tweet in train_df[\"clean_content\"]])\n",
    "y_train_w2v = np.array(train_df[\"target\"], dtype=np.float32)\n",
    "X_test_w2v = np.array([text_to_sequence(tweet, W2V_VOCAB_DICT) for tweet in test_df[\"clean_content\"]])\n",
    "y_test_w2v = np.array(test_df[\"target\"], dtype=np.float32)\n",
    "\n",
    "print(f\"X_train_w2v shape: {X_train_w2v.shape}, y_train shape_w2v: {y_train_w2v.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_ft = np.array([text_to_sequence(tweet, FT_VOCAB_DICT) for tweet in train_df[\"clean_content\"]])\n",
    "y_train_ft = np.array(train_df[\"target\"], dtype=np.float32)\n",
    "X_test_ft = np.array([text_to_sequence(tweet, FT_VOCAB_DICT) for tweet in test_df[\"clean_content\"]])\n",
    "y_test_ft = np.array(test_df[\"target\"], dtype=np.float32)\n",
    "\n",
    "print(f\"X_train_w2v shape: {X_train_ft.shape}, y_train shape_w2v: {y_train_ft.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_embedding_matrix = np.zeros((W2V_VOCAB_SIZE, EMBEDDING_DIM))\n",
    "\n",
    "for word, i in W2V_VOCAB_DICT.items():\n",
    "\tif word in w2v_model.wv:\n",
    "\t\tw2v_embedding_matrix[i] = w2v_model.wv[word]\n",
    "\telif word not in ['<UNK>', '<PAD>']:\n",
    "\t\tw2v_embedding_matrix[i] = np.random.uniform(low=-0.6, high=0.6, size=(EMBEDDING_DIM,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_embedding_matrix = np.zeros((FT_VOCAB_SIZE, EMBEDDING_DIM))\n",
    "\n",
    "for word, i in FT_VOCAB_DICT.items():\n",
    "\tif word in ft_model.wv:\n",
    "\t\tft_embedding_matrix[i] = ft_model.wv[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"KERAS_BACKEND\"] = \"torch\"\n",
    "import keras\n",
    "import torch\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def create_lstm_model(hidden_dim, n_layers, dropout, learning_rate, vocab_size, embedding_matrix, trainable):\n",
    "\n",
    "\tinputs = keras.Input(shape=(MAX_SEQ_LEN,), dtype=\"int32\")\n",
    "\t\n",
    "\tembedding_layer = keras.layers.Embedding(\n",
    "\t\tinput_dim=vocab_size,\n",
    "\t\toutput_dim=EMBEDDING_DIM,\n",
    "\t\tweights=[embedding_matrix],\n",
    "\t\ttrainable=trainable,\n",
    "\t\tmask_zero=True\n",
    "\t)\n",
    "\tx = embedding_layer(inputs)\n",
    "\t\n",
    "\tx = keras.layers.Dropout(dropout)(x)\n",
    "\n",
    "\tfor i in range(n_layers - 1):\n",
    "\t\tx = keras.layers.Bidirectional(\n",
    "\t\t\tkeras.layers.LSTM(hidden_dim, return_sequences=True, dropout=dropout)\n",
    "\t\t)(x)\n",
    "\n",
    "\tx = keras.layers.Bidirectional(\n",
    "\t\tkeras.layers.LSTM(hidden_dim, return_sequences=False, dropout=dropout)\n",
    "\t)(x)\n",
    "\t\n",
    "\tx = keras.layers.Dropout(dropout)(x)\n",
    "\toutputs = keras.layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "\t\n",
    "\tmodel = keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "\tmodel.compile(\n",
    "\t\toptimizer=keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "\t\tloss=\"binary_crossentropy\",\n",
    "\t\tmetrics=[\"accuracy\"]\n",
    "\t)\n",
    "\n",
    "\treturn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "mlflow.autolog(disable=True)\n",
    "\n",
    "HIDDEN_DIM = 256\n",
    "N_LAYERS = 2\n",
    "DROPOUT = 0.2\n",
    "LEARNING_RATE = 1e-3\n",
    "N_EPOCHS = 3\n",
    "BATCH_SIZE = 4096\n",
    "\n",
    "\n",
    "experiment_name = \"Réalisez une analyse de sentiments grâce au Deep Learning\"\n",
    "mlflow.set_experiment(experiment_name=experiment_name)\n",
    "\n",
    "\n",
    "# w2v_lstm_model = create_lstm_model(HIDDEN_DIM, N_LAYERS, DROPOUT, LEARNING_RATE, W2V_VOCAB_SIZE, w2v_embedding_matrix)\n",
    "\n",
    "# with mlflow.start_run(run_name=\"Word2Vec Test run\") as run:\n",
    "\n",
    "# \thistory = w2v_lstm_model.fit(\n",
    "# \t\tX_train_w2v,\n",
    "# \t\ty_train_w2v,\n",
    "# \t\tepochs=N_EPOCHS,\n",
    "# \t\tbatch_size=BATCH_SIZE,\n",
    "# \t\tvalidation_data=(X_test_w2v, y_test_w2v),\n",
    "# \t\tverbose=1\n",
    "# \t)\n",
    "\n",
    "# \thistory_dict = history.history\n",
    "\n",
    "# \tfor epoch in range(N_EPOCHS):\n",
    "\n",
    "# \t\tif 'loss' in history_dict:\n",
    "# \t\t\tmlflow.log_metric(\"train_loss\", history_dict['loss'][epoch], step=epoch + 1)\n",
    "# \t\tif 'accuracy' in history_dict:\n",
    "# \t\t\tmlflow.log_metric(\"train_acc\", history_dict['accuracy'][epoch], step=epoch + 1)\n",
    "\t\t\t\n",
    "# \t\tif 'val_loss' in history_dict:\n",
    "# \t\t\tmlflow.log_metric(\"valid_loss\", history_dict['val_loss'][epoch], step=epoch + 1)\n",
    "# \t\tif 'val_accuracy' in history_dict:\n",
    "# \t\t\tmlflow.log_metric(\"valid_acc\", history_dict['val_accuracy'][epoch], step=epoch + 1)\n",
    "\t\n",
    "# \tloss, acc = w2v_lstm_model.evaluate(X_test_w2v, y_test_w2v, batch_size=BATCH_SIZE, verbose=0)\n",
    "\n",
    "# \tmlflow.log_metric(\"final_test_loss\", loss)\n",
    "# \tmlflow.log_metric(\"final_test_acc\", acc)\n",
    "\n",
    "# \tinput_example = X_test_w2v[:1]\n",
    "# \toutput_example = keras.ops.convert_to_numpy(w2v_lstm_model.predict(input_example, verbose=0))\n",
    "\n",
    "# \tmlflow.keras.log_model(\n",
    "# \t\tw2v_lstm_model,\n",
    "# \t\tname=\"lstm\",\n",
    "# \t\tsignature=mlflow.models.signature.infer_signature(input_example, output_example),\n",
    "# \t)\n",
    "\n",
    "# \tparams = {\n",
    "# \t\t\"model_type\": \"Keras_LSTM\",\n",
    "# \t\t\"keras_backend\": keras.backend.backend(),\n",
    "# \t\t\"hidden_dim\": HIDDEN_DIM,\n",
    "# \t\t\"n_layers\": N_LAYERS,\n",
    "# \t\t\"dropout\": DROPOUT,\n",
    "# \t\t\"learning_rate\": LEARNING_RATE,\n",
    "# \t\t\"n_epochs\": N_EPOCHS,\n",
    "# \t\t\"batch_size\": BATCH_SIZE,\n",
    "# \t\t\"embedding_dim\": EMBEDDING_DIM,\n",
    "# \t\t\"vocab_size\": W2V_VOCAB_SIZE,\n",
    "# \t\t\"trainable_embeddings\": w2v_lstm_model.layers[1].trainable,\n",
    "# \t}\n",
    "# \tmlflow.log_params(params)\n",
    "\n",
    "# \tprint(f'Test Loss: {loss:.3f} | Test Acc: {acc*100:.2f}%')\n",
    "# \ttraining = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ft_lstm_model = create_lstm_model(HIDDEN_DIM, N_LAYERS, DROPOUT, LEARNING_RATE, FT_VOCAB_SIZE, ft_embedding_matrix)\n",
    "\n",
    "# with mlflow.start_run(run_name=\"FastText Test run\") as run:\n",
    "\n",
    "# \thistory = ft_lstm_model.fit(\n",
    "# \t\tX_train_ft,\n",
    "# \t\ty_train_ft,\n",
    "# \t\tepochs=N_EPOCHS,\n",
    "# \t\tbatch_size=BATCH_SIZE,\n",
    "# \t\tvalidation_data=(X_test_ft, y_test_ft),\n",
    "# \t\tverbose=1\n",
    "# \t)\n",
    "\n",
    "# \thistory_dict = history.history\n",
    "\n",
    "# \tfor epoch in range(N_EPOCHS):\n",
    "\n",
    "# \t\tif 'loss' in history_dict:\n",
    "# \t\t\tmlflow.log_metric(\"train_loss\", history_dict['loss'][epoch], step=epoch + 1)\n",
    "# \t\tif 'accuracy' in history_dict:\n",
    "# \t\t\tmlflow.log_metric(\"train_acc\", history_dict['accuracy'][epoch], step=epoch + 1)\n",
    "\t\t\t\n",
    "# \t\tif 'val_loss' in history_dict:\n",
    "# \t\t\tmlflow.log_metric(\"valid_loss\", history_dict['val_loss'][epoch], step=epoch + 1)\n",
    "# \t\tif 'val_accuracy' in history_dict:\n",
    "# \t\t\tmlflow.log_metric(\"valid_acc\", history_dict['val_accuracy'][epoch], step=epoch + 1)\n",
    "\t\n",
    "# \tloss, acc = ft_lstm_model.evaluate(X_test_ft, y_test_ft, batch_size=BATCH_SIZE, verbose=0)\n",
    "\n",
    "# \tmlflow.log_metric(\"final_test_loss\", loss)\n",
    "# \tmlflow.log_metric(\"final_test_acc\", acc)\n",
    "\n",
    "# \tinput_example = X_test_ft[:1]\n",
    "# \toutput_example = keras.ops.convert_to_numpy(ft_lstm_model.predict(input_example, verbose=0))\n",
    "\n",
    "# \tmlflow.keras.log_model(\n",
    "# \t\tft_lstm_model,\n",
    "# \t\tname=\"lstm\",\n",
    "# \t\tsignature=mlflow.models.signature.infer_signature(input_example, output_example),\n",
    "# \t)\n",
    "\n",
    "# \tparams = {\n",
    "# \t\t\"model_type\": \"Keras_LSTM\",\n",
    "# \t\t\"keras_backend\": keras.backend.backend(),\n",
    "# \t\t\"hidden_dim\": HIDDEN_DIM,\n",
    "# \t\t\"n_layers\": N_LAYERS,\n",
    "# \t\t\"dropout\": DROPOUT,\n",
    "# \t\t\"learning_rate\": LEARNING_RATE,\n",
    "# \t\t\"n_epochs\": N_EPOCHS,\n",
    "# \t\t\"batch_size\": BATCH_SIZE,\n",
    "# \t\t\"embedding_dim\": EMBEDDING_DIM,\n",
    "# \t\t\"vocab_size\": FT_VOCAB_SIZE,\n",
    "# \t\t\"trainable_embeddings\": ft_lstm_model.layers[1].trainable,\n",
    "# \t}\n",
    "# \tmlflow.log_params(params)\n",
    "\n",
    "# \tprint(f'Test Loss: {loss:.3f} | Test Acc: {acc*100:.2f}%')\n",
    "# \ttraining = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "from itertools import product\n",
    "import tensorflow as tf\n",
    "\n",
    "N_EPOCHS = 5\n",
    "MIN_BATCH_SIZE = 64\n",
    "\n",
    "HIDDEN_DIM = [1024]\n",
    "N_LAYERS = [5]\n",
    "DROPOUT = [0.2]\n",
    "LEARNING_RATE = [1e-3, 1e-2]\n",
    "TRAINABLE = [\"False\", \"True\"]\n",
    "\n",
    "all_params = list(product(HIDDEN_DIM, N_LAYERS, DROPOUT, LEARNING_RATE, TRAINABLE))\n",
    "max_runs = len(all_params)\n",
    "counter = 0\n",
    "\n",
    "\n",
    "with mlflow.start_run(run_name=\"Word2Vec\", run_id=\"140227e789ff49378183cabff82bac58\") as parent_run:\n",
    "\n",
    "\tfor hd, nl, dp, lr, tr in all_params:\n",
    "\n",
    "\t\tcounter += 1\n",
    "\t\tprint(f\"Run numéro {counter}/{max_runs} -> HD:{hd}-NL:{nl}-DP:{dp}-LR:{lr}-TRAINABLE:{tr}\")\n",
    "\t\t\n",
    "\t\tBATCH_SIZE=128\n",
    "\n",
    "\t\ttraining = 1\n",
    "\t\twhile training:\n",
    "\t\t\ttry:\n",
    "\n",
    "\t\t\t\tif 'model' in locals():\n",
    "\t\t\t\t\tdel model\n",
    "\t\t\t\tkeras.backend.clear_session()\n",
    "\t\t\t\tgc.collect()\n",
    "\t\t\t\ttorch.cuda.empty_cache()\n",
    "\n",
    "\t\t\t\tmodel = create_lstm_model(hd, nl, dp, lr, W2V_VOCAB_SIZE, w2v_embedding_matrix, tr)\n",
    "\t\t\t\trun_name = f\"HD:{hd}-NL:{nl}-DP:{dp}-LR:{lr}-TRAINABLE:{tr}\"\n",
    "\n",
    "\n",
    "\t\t\t\twith mlflow.start_run(nested=True, run_name=run_name) as run:\n",
    "\n",
    "\n",
    "\t\t\t\t\ttrain_dataset = tf.data.Dataset.from_tensor_slices((X_train_w2v, y_train_w2v))\n",
    "\t\t\t\t\ttrain_dataset = train_dataset.shuffle(buffer_size=1024).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "\t\t\t\t\ttest_dataset = tf.data.Dataset.from_tensor_slices((X_test_w2v, y_test_w2v))\n",
    "\t\t\t\t\ttest_dataset = test_dataset.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "\t\t\t\t\thistory = model.fit(\n",
    "\t\t\t\t\t\ttrain_dataset,\n",
    "\t\t\t\t\t\tepochs=N_EPOCHS,\n",
    "\t\t\t\t\t\tbatch_size=BATCH_SIZE,\n",
    "\t\t\t\t\t\tvalidation_data=test_dataset,\n",
    "\t\t\t\t\t\tverbose=1\n",
    "\t\t\t\t\t)\n",
    "\n",
    "\t\t\t\t\thistory_dict = history.history\n",
    "\n",
    "\t\t\t\t\tfor epoch in range(N_EPOCHS):\n",
    "\n",
    "\t\t\t\t\t\tif 'loss' in history_dict:\n",
    "\t\t\t\t\t\t\tmlflow.log_metric(\"train_loss\", history_dict['loss'][epoch], step=epoch + 1)\n",
    "\t\t\t\t\t\tif 'accuracy' in history_dict:\n",
    "\t\t\t\t\t\t\tmlflow.log_metric(\"train_acc\", history_dict['accuracy'][epoch], step=epoch + 1)\n",
    "\t\t\t\t\t\t\t\n",
    "\t\t\t\t\t\tif 'val_loss' in history_dict:\n",
    "\t\t\t\t\t\t\tmlflow.log_metric(\"valid_loss\", history_dict['val_loss'][epoch], step=epoch + 1)\n",
    "\t\t\t\t\t\tif 'val_accuracy' in history_dict:\n",
    "\t\t\t\t\t\t\tmlflow.log_metric(\"valid_acc\", history_dict['val_accuracy'][epoch], step=epoch + 1)\n",
    "\t\t\t\t\t\n",
    "\t\t\t\t\tloss, acc = model.evaluate(X_test_w2v, y_test_w2v, batch_size=BATCH_SIZE, verbose=0)\n",
    "\n",
    "\t\t\t\t\tmlflow.log_metric(\"final_test_loss\", loss)\n",
    "\t\t\t\t\tmlflow.log_metric(\"final_test_acc\", acc)\n",
    "\n",
    "\t\t\t\t\tinput_example = X_test_w2v[:1]\n",
    "\t\t\t\t\toutput_example = keras.ops.convert_to_numpy(model.predict(input_example, verbose=0))\n",
    "\n",
    "\t\t\t\t\tmlflow.keras.log_model(\n",
    "\t\t\t\t\t\tmodel,\n",
    "\t\t\t\t\t\tname=\"word2vec\",\n",
    "\t\t\t\t\t\tsignature=mlflow.models.signature.infer_signature(input_example, output_example),\n",
    "\t\t\t\t\t)\n",
    "\n",
    "\t\t\t\t\tparams = {\n",
    "\t\t\t\t\t\t\"model_type\": \"Keras_LSTM_Word2Vec\",\n",
    "\t\t\t\t\t\t\"keras_backend\": keras.backend.backend(),\n",
    "\t\t\t\t\t\t\"hidden_dim\": hd,\n",
    "\t\t\t\t\t\t\"n_layers\": nl,\n",
    "\t\t\t\t\t\t\"dropout\": dp,\n",
    "\t\t\t\t\t\t\"learning_rate\": lr,\n",
    "\t\t\t\t\t\t\"n_epochs\": N_EPOCHS,\n",
    "\t\t\t\t\t\t\"batch_size\": BATCH_SIZE,\n",
    "\t\t\t\t\t\t\"embedding_dim\": EMBEDDING_DIM,\n",
    "\t\t\t\t\t\t\"vocab_size\": W2V_VOCAB_SIZE,\n",
    "\t\t\t\t\t\t\"trainable_embeddings\": model.layers[1].trainable,\n",
    "\t\t\t\t\t}\n",
    "\t\t\t\t\tmlflow.log_params(params)\n",
    "\n",
    "\t\t\t\t\tprint(f'Test Loss: {loss:.3f} | Test Acc: {acc*100:.2f}%')\n",
    "\t\t\t\t\ttraining = 0\n",
    "\n",
    "\t\t\texcept RuntimeError:\n",
    "\t\t\t\t\n",
    "\t\t\t\tprint(f\"\\nOOM/MemoryError à BATCH_SIZE={BATCH_SIZE}.\")\n",
    "\t\t\t\t\n",
    "\t\t\t\trun_id = run.info.run_id\n",
    "\t\t\t\tmlflow.delete_run(run_id)\n",
    "\n",
    "\t\t\t\tif BATCH_SIZE <= MIN_BATCH_SIZE:\n",
    "\t\t\t\t\tprint(f\"\\nAbandon de la configuration : BATCH_SIZE minimum ({MIN_BATCH_SIZE}) atteint.\")\n",
    "\t\t\t\t\ttraining = 0\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tBATCH_SIZE = int(BATCH_SIZE / 2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IAEngP7-oYMYfi1b",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
